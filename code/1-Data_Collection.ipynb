{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5: Leveraging Social Media to Map Disasters\n",
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: {'User-Agent': 'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_6; de-de) AppleWebKit/533.20.25 (KHTML, like Gecko) Version/5.0.4 Safari/533.20.27'}\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from twitterscraper import query_tweets\n",
    "import pandas as pd\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each member of the team was responsible for gathering tweets on different natural disasters. The natural disasters that are included in the data collection was: \n",
    "- Hurricane Harvey \n",
    "- Major floods during July 2019\n",
    "- Montecito mudslides of January 2018\n",
    "- Noreaster of March 2018\n",
    "- Tornado outbreak during April 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hurricane Harvey Tweets\n",
    "#### Using key words #hurricaneharvey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_date = dt.date(2017,8,25)\n",
    "end_date = dt.date(2017,8,26)\n",
    "\n",
    "limit = 3000\n",
    "\n",
    "lang = \"english\"\n",
    "\n",
    "tweets = query_tweets(\"#hurricaneharvey\",\n",
    "                      begindate=begin_date,\n",
    "                      enddate=end_date,\n",
    "                      limit=limit,\n",
    "                      lang=lang)\n",
    "hh = pd.DataFrame(t.__dict__ for t in tweets) \n",
    "hh.to_csv('hurricane_harvey_general.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run above cell four times in order to get enough data for the time frame during Hurricane Harvey. Increase date range by 2 every iteration. Merge data frames as one and export as csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flood Tweets\n",
    "#### Using key words flood, flooding or floods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_date = dt.date(2019,7,21)\n",
    "end_date = dt.date(2019,7,22)\n",
    "\n",
    "limit = 3000\n",
    "\n",
    "lang = \"english\"\n",
    "\n",
    "tweets = query_tweets(\"flood OR flooding OR floods\",\n",
    "                      begindate=begin_date,\n",
    "                      enddate=end_date,\n",
    "                      limit=limit,\n",
    "                      lang='english')\n",
    "df = pd.DataFrame(t.__dict__ for t in tweets)\n",
    "df.to_csv(\"flood_072119-072219.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tornado Tweets\n",
    "#### Using key words tornado or #tornado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_date = dt.date(2019,4,13)\n",
    "end_date = dt.date(2019,4,15)\n",
    "\n",
    "limit = 3000\n",
    "\n",
    "lang = \"english\"\n",
    "\n",
    "tweets = query_tweets(\"tornado OR #tornado\",\n",
    "                      begindate=begin_date,\n",
    "                      enddate=end_date,\n",
    "                      limit=limit,\n",
    "                      lang='english')\n",
    "df = pd.DataFrame(t.__dict__ for t in tweets)\n",
    "df.to_csv('tornados_41319-41519.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noreaster Tweets\n",
    "#### Using key words noreaster or #noreaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_date = dt.date(2018,3,1)\n",
    "end_date = dt.date(2018,3,5)\n",
    "\n",
    "limit = 3000\n",
    "\n",
    "lang = \"english\"\n",
    "\n",
    "tweets = query_tweets(\"noreaster OR #noreaster\",\n",
    "                      begindate=begin_date,\n",
    "                      enddate=end_date,\n",
    "                      limit=limit,\n",
    "                      lang='english')\n",
    "df = pd.DataFrame(t.__dict__ for t in tweets)\n",
    "df.to_csv('noreaster_030118-030518.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mudslide Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using key words mudslide, mudslides or montecito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_date = dt.date(2018,1,8)\n",
    "end_date = dt.date(2018,1,12)\n",
    "limit = 3000\n",
    "lang = \"english\"\n",
    "\n",
    "tweets = query_tweets(\"montecito OR mudslide OR mudslides\",\n",
    "                      begindate=begin_date,\n",
    "                      enddate=end_date,\n",
    "                      limit=limit,\n",
    "                      lang=lang)\n",
    "df.to_csv('mudslides.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
