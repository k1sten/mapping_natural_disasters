{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5: Leveraging Social Media to Map Disasters\n",
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: {'User-Agent': 'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_6; de-de) AppleWebKit/533.20.25 (KHTML, like Gecko) Version/5.0.4 Safari/533.20.27'}\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from twitterscraper import query_tweets\n",
    "import pandas as pd\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each member of the team was responsible for gathering tweets on different natural disasters. The natural disasters that are included in the data collection was: \n",
    "- Hurricane Harvey \n",
    "- Major floods during July 2019\n",
    "- Montecito mudslides of January 2018\n",
    "- Noreaster of March 2018\n",
    "- Tornado outbreak during April 2019\n",
    "\n",
    "Each of these incidents represents a different type of natural disaster. This wide range of disasters was specifically chosen so that the model could be trained on many different types of emergencies. This should give us a model that is able to be used in many different situations, and not just a single type of natural disaster. Each specific incident was chosen for its timeliness and the accompanying use of Twitter during those events. For Hurricane Harvey, there was even a specific hashtag for people to use when tweeting about it. As the use of Twitter has been increasing over time, we wanted to get the most amount of possible results, and chose disasters that were fairly recent, capturing this upward trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hurricane Harvey Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THis search is going to be using the key word #hurricaneharvey. The date range covers the two days that the hurricane was most active in the Houston area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_date = dt.date(2017,8,25)\n",
    "end_date = dt.date(2017,8,26)\n",
    "\n",
    "limit = 3000\n",
    "\n",
    "lang = \"english\"\n",
    "\n",
    "tweets = query_tweets(\"#hurricaneharvey\",\n",
    "                      begindate=begin_date,\n",
    "                      enddate=end_date,\n",
    "                      limit=limit,\n",
    "                      lang=lang)\n",
    "hh = pd.DataFrame(t.__dict__ for t in tweets) \n",
    "hh.to_csv('hurricane_harvey_general.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run above cell four times in order to get enough data for the time frame during Hurricane Harvey. Increase date range by 2 every iteration. Merge data frames as one and export as csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flood Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This search is going be using the key words flood, flooding or floods. The date range was specified to be the two days that covered the highest amount of rainfall and accompanying floods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_date = dt.date(2019,7,21)\n",
    "end_date = dt.date(2019,7,22)\n",
    "\n",
    "limit = 3000\n",
    "\n",
    "lang = \"english\"\n",
    "\n",
    "tweets = query_tweets(\"flood OR flooding OR floods\",\n",
    "                      begindate=begin_date,\n",
    "                      enddate=end_date,\n",
    "                      limit=limit,\n",
    "                      lang='english')\n",
    "df = pd.DataFrame(t.__dict__ for t in tweets)\n",
    "df.to_csv(\"flood_072119-072219.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tornado Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This search is going to be using key words tornado or #tornado. The date range targets the three days where there was the highest amount of tornado activity in the southern US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_date = dt.date(2019,4,13)\n",
    "end_date = dt.date(2019,4,15)\n",
    "\n",
    "limit = 3000\n",
    "\n",
    "lang = \"english\"\n",
    "\n",
    "tweets = query_tweets(\"tornado OR #tornado\",\n",
    "                      begindate=begin_date,\n",
    "                      enddate=end_date,\n",
    "                      limit=limit,\n",
    "                      lang='english')\n",
    "df = pd.DataFrame(t.__dict__ for t in tweets)\n",
    "df.to_csv('tornados_41319-41519.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noreaster Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This search is going to be using key words noreaster or #noreaster. The date range represents the five days where there was high storm activity and following days of issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_date = dt.date(2018,3,1)\n",
    "end_date = dt.date(2018,3,5)\n",
    "\n",
    "limit = 3000\n",
    "\n",
    "lang = \"english\"\n",
    "\n",
    "tweets = query_tweets(\"noreaster OR #noreaster\",\n",
    "                      begindate=begin_date,\n",
    "                      enddate=end_date,\n",
    "                      limit=limit,\n",
    "                      lang='english')\n",
    "df = pd.DataFrame(t.__dict__ for t in tweets)\n",
    "df.to_csv('noreaster_030118-030518.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mudslide Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This search is going to be using key words mudslide, mudslides or montecito. The date range covers the day leading up to the start of the mudslides, and the days following the main mudslide activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_date = dt.date(2018,1,8)\n",
    "end_date = dt.date(2018,1,12)\n",
    "limit = 3000\n",
    "lang = \"english\"\n",
    "\n",
    "tweets = query_tweets(\"montecito OR mudslide OR mudslides\",\n",
    "                      begindate=begin_date,\n",
    "                      enddate=end_date,\n",
    "                      limit=limit,\n",
    "                      lang=lang)\n",
    "df.to_csv('mudslides.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://blog.twitter.com/en_sea/topics/insights/2018/5-Tips-for-using-Twitter-during-emergencies-and-natural-disaster.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
